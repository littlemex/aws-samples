{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkによる特徴処理、XGBoostによる学習、Inference Pipelineとしての展開\n",
    "\n",
    "一般的に、機械学習（ML）のプロセスは、様々なETLジョブによるデータの収集、データの前処理、標準的な技術や事前知識を取り入れたデータセットのフェザリング、そして最終的にアルゴリズムを使用したMLモデルのトレーニングといういくつかのステップで構成されています。\n",
    "\n",
    "多くの場合、学習されたモデルがリアルタイムまたはバッチの予測要求の処理に使用される場合、モデルはアルゴリズムに渡す前に前処理（例：フェザリング）を必要とするフォーマットでデータを受け取ります。次のノートブックでは、Spark Feature TransformersとSageMaker XGBoostアルゴリズムを活用してMLパイプラインを構築し、モデルをトレーニングした後、パイプライン（Feature TransformerとXGBoost）を単一のエンドポイントの後ろにInference Pipelineとして配置し、リアルタイム推論やAmazon SageMaker Batch Transformを使用したバッチ推論を行う方法を示します。\n",
    "\n",
    "このノートブックでは、サーバーレスSparkを実行するためにAmazon Glueを使用しています。このノートブックでは、小さなデータセットでエンドツーエンドのフローを実演していますが、このセットアップは、より大きなデータセットにシームレスに使用することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的: アワビの物理的な測定値から年齢を予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このデータセットは[UCI Machine Learning](https://archive.ics.uci.edu/ml/datasets/abalone)から入手できます。この課題の目的は、アワビ（貝の一種）の物理的な測定値から年齢を決定することです。基本的には、回帰問題です。データセットにはいくつかの特徴が含まれています - `sex` (categorical), `length` (continuous), `diameter` (continuous), `height` (continuous), `whole_weight` (continuous), `shucked_weight` (continuous), `viscera_weight` (continuous), `shell_weight` (continuous), `rings` (integer).我々の目標は、年齢の良い近似値である変数 `rings` を予測することです (年齢は `rings` + 1.5). \n",
    "\n",
    "SparkMLを使ってデータセットを処理し（1つまたは複数の特徴変換器を適用）、変換されたデータセットをS3にアップロードして、XGBoostでのトレーニングに使用できるようにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法論\n",
    "このノートブックは、いくつかのハイレベルなステップで構成されています。\n",
    "\n",
    "* AWS Glue を使用して SparkML 特徴処理ジョブを実行する。\n",
    "* SageMaker XGBoost を使用して、SparkML ジョブで生成された処理済みデータセットでトレーニングを行う。\n",
    "* SparkMLとXGBoostモデルからなる推論パイプラインを構築し、リアルタイム推論エンドポイントとする。\n",
    "* 単一のBatch TransformジョブのためのSparkMLとXGBoostモデルからなる推論パイプラインの構築。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkMLジョブの実行にAWS Glueを使用する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は[AWS Glue](https://aws.amazon.com/glue)を使ってSparkMLのジョブを実行します。AWS GlueはサーバーレスのETLサービスで、標準的なSpark/PySparkのジョブを実行するのに使用できます。Glueは現在、`Python 2.7`しかサポートしていないので、スクリプトは`Python 2.7`で書きます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## このノートブックからAWS Glueを起動するためのパーミッション設定\n",
    "このノートブックでAWS Glueのジョブを実行できるようにするためには、このノートブックのデフォルトの実行ロールに1つの追加パーミッションを追加する必要があります。ここではSageMaker Python SDKを使ってデフォルトの実行ロールを取得し、[IAM Dashboard](https://console.aws.amazon.com/iam/home)でロールを編集してAWS Glue固有のパーミッションを追加する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノートブックの現在の実行ロールを知る\n",
    "SageMaker Python SDKを使用して、強化が必要なこのノートブックの現在のロールを取得しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SageMaker Python SDK to get the Session and execution_role\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "print(role[role.rfind(\"/\") + 1 :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### このロールに追加の信頼できるエンティティとしてAWS Glueを追加する\n",
    "この手順は、追加の**Role**を作成することなく、Glue APIの呼び出し時にもこのノートブックの実行ロールを渡したい場合に必要です。AWS Glueを使用したことがない場合は、このステップは必須です。\n",
    "\n",
    "もし以前にAWS Glueを使用したことがあるのであれば、Glue APIの呼び出しに使用できる既存のロールがあるはずです。その場合は、Glueを呼び出す際にそのロールを渡すことができ（このノートブックで後述）、この次のステップをスキップすることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IAMダッシュボードで、左サイドナビの**Roles**をクリックして、このRoleを検索してください。Roleが表示されたら、Roleをクリックして、その**Summary**ページに移動します。Summary**ページの**Trust relationships**タブをクリックして、AWS Glueを追加の信頼できるエンティティとして追加します。\n",
    "\n",
    "**信頼関係の編集** をクリックして、JSONをこのJSONで置き換えます。\n",
    "\n",
    "```\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": [\n",
    "          \"sagemaker.amazonaws.com\",\n",
    "          \"glue.amazonaws.com\"\n",
    "        ]\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "これが完了したら、**Update Trust Policy**をクリックすれば完了です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットのダウンロードとS3へのアップロード\n",
    "\n",
    "SageMakerチームはUCIからデータセットをダウンロードし、我々のアカウントのS3バケットの一つにアップロードしました。このノートでは、そのバケットからダウンロードし、あなたのバケットにアップロードして、AWS Glueがデータにアクセスできるようにします。先ほど追加したデフォルトのAWS Glueのパーミッションでは、データが `aws-glue` という文字列のバケットに存在することを想定しています。そのため、データセットをダウンロードした後、あなたのアカウントに有効な名前でS3バケットを作成し、データをS3にアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/abalone/abalone.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3バケットの作成とデータセットのアップロード\n",
    "次に、`aws-glue`という文字列を名前に持つS3バケットを作成し、このデータをS3バケットにアップロードします。AWS Glue経由でSparkジョブを実行するために既存のバケットを使用したい場合、`Role`がそのバケットからのアップロードとダウンロードのアクセス権限を持っていれば、そのバケットを使用してデータをアップロードすることができます。\n",
    "\n",
    "バケットが作成されると、以下のセルでは、このバケットにローカルにダウンロードされた `abalone.csv` ファイルも `input/abalone` というプレフィックスで更新されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "boto_session = sess.boto_session\n",
    "s3 = boto_session.resource(\"s3\")\n",
    "account = boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto_session.region_name\n",
    "default_bucket = \"aws-glue-{}-{}\".format(account, region)\n",
    "\n",
    "try:\n",
    "    if region == \"us-east-1\":\n",
    "        s3.create_bucket(Bucket=default_bucket)\n",
    "    else:\n",
    "        s3.create_bucket(\n",
    "            Bucket=default_bucket, CreateBucketConfiguration={\"LocationConstraint\": region}\n",
    "        )\n",
    "except ClientError as e:\n",
    "    error_code = e.response[\"Error\"][\"Code\"]\n",
    "    message = e.response[\"Error\"][\"Message\"]\n",
    "    if error_code == \"BucketAlreadyOwnedByYou\":\n",
    "        print(\"A bucket with the same name already exists in your account - using the same bucket.\")\n",
    "        pass\n",
    "\n",
    "# Uploading the training data to S3\n",
    "sess.upload_data(path=\"abalone.csv\", bucket=default_bucket, key_prefix=\"input/abalone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkMLを用いた特徴処理スクリプトの作成\n",
    "\n",
    "SparkMLを使った特徴量変換のコードは、同じディレクトリに書かれた`abalone_processing.py`ファイルにあります。コードを見てみると、SparkMLの標準的な構造を使ってデータを特徴付けするパイプラインを定義していることがわかります。\n",
    "\n",
    "Spark ML Pipelineの「fit」と「transform」が完了したら、スクリプトの一部としてデータセットを80対20のトレーニング用と検証用に分割し、S3にアップロードしてXGBoostのトレーニングに使用できるようにしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習したSpark MLモデルを[MLeap](https://github.com/combust/mleap)でシリアル化\n",
    "Apache Sparkはバッチ処理のワークロードに最も適しています。低レイテンシーで推論するためには、MLeapライブラリを使ってMLeapバンドルにシリアライズし、後に[SageMaker SparkML Serving](https://github.com/aws/sagemaker-sparkml-serving-container)を使ってリアルタイムおよびバッチ推論を行う必要があります。\n",
    "\n",
    "スクリプトの中でMLeapの`SerializeToBundle()`メソッドを使うことで、ML PipelineをMLeap bundleにシリアライズし、SageMakerが期待する`tar.gz`形式でS3にアップロードしています。\n",
    "\n",
    "追記：[参考Qiita](https://qiita.com/souchan-t/items/3bdbd39333852416a9c7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glueのためにコードとその他の依存関係をS3にアップロードする\n",
    "SageMakerとは異なり、AWS Glueでコードを実行するためには、Dockerイメージを用意する必要はありません。コードと依存関係を直接S3にアップロードし、Glueジョブを起動する際にそれらの場所を渡すことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkMLスクリプトをS3にアップロード\n",
    "今から`abalone_processing.py`スクリプトをS3にアップロードして、Glueがそれを使ってPySparkのジョブを実行できるようにします。必要に応じて、独自のスクリプトに置き換えることができます。コードに複数のファイルがある場合は、ここで行っているように1つのファイルをアップロードするのではなく、それらのファイルをzipで圧縮してS3にアップロードする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_location = sess.upload_data(\n",
    "    path=\"abalone_processing.py\", bucket=default_bucket, key_prefix=\"codes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3にMLeapの依存ファイルをアップロード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回の作業では、MLeapの依存関係もGlueに渡さなければなりません。MLeapはデフォルトのSparkにバンドルされていない追加ライブラリです。\n",
    "\n",
    "Sparkエコシステムのほとんどのパッケージと同様に、MLeapもScalaパッケージとして実装されており、PySparkから利用できるようにPythonで書かれたフロントエンドラッパーが付いています。MLeapのPythonライブラリとJARがGlueのジョブ環境で利用可能であることを確認する必要があります。以下のセルでは、SageMakerがホスティングしているバケットからMLeap Pythonの依存関係とJARをダウンロードし、上記で作成したS3バケットにアップロードしています。\n",
    "\n",
    "もし、コードに`nltk`などの他のPythonライブラリを使用している場合は、PyPIからwheelファイルをダウンロードして、同様にS3にアップロードする必要があります。現時点では、Glueは純粋なPythonライブラリをこの方法で渡すことしかサポートしていません（例えば、`Pandas`や`OpenCV`を渡すことはできません）。ただし、`NumPy` や `SciPy` は Glue 環境にあらかじめインストールされているので、これらをパッケージとして渡す必要はありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/python/python.zip\n",
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_dep_location = sess.upload_data(\n",
    "    path=\"python.zip\", bucket=default_bucket, key_prefix=\"dependencies/python\"\n",
    ")\n",
    "jar_dep_location = sess.upload_data(\n",
    "    path=\"mleap_spark_assembly.jar\", bucket=default_bucket, key_prefix=\"dependencies/jar\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データやモデルの出力先を決める\n",
    "次に、変換されたデータセットがアップロードされるべき出力場所を定義します。また、MLeapのシリアル化されたモデルが更新されるモデルの場所も指定します。この場所は、AWS Glue ライブラリの `getResolvedOptions` メソッドを使って Spark スクリプトの一部として消費されます (詳細は `abalone_processing.py` を参照してください)。\n",
    "\n",
    "このようにコードを設計することで、このノートブックからSageMakerの他の操作の一部として、これらの変数を再利用することができます（詳細は後述）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# Input location of the data, We uploaded our train.csv file to input key previously\n",
    "s3_input_bucket = default_bucket\n",
    "s3_input_key_prefix = \"input/abalone\"\n",
    "\n",
    "# Output location of the data. The input data will be split, transformed, and\n",
    "# uploaded to output/train and output/validation\n",
    "s3_output_bucket = default_bucket\n",
    "s3_output_key_prefix = timestamp_prefix + \"/abalone\"\n",
    "\n",
    "# the MLeap serialized SparkML model will be uploaded to output/mleap\n",
    "s3_model_bucket = default_bucket\n",
    "s3_model_key_prefix = s3_output_key_prefix + \"/mleap\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue APIの呼び出し"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、Boto経由でGlueクライアントを作成し、Glueの`create_job` APIを呼び出すことができるようにします。create_job` APIは、Glueでジョブを実行するのに使用できるジョブ定義を作成します。ここで作成されるジョブ定義は可変型です。ジョブを作成する際には、コードの場所と依存関係の場所もGlueに渡しています。\n",
    "\n",
    "AllocatedCapacity`パラメータは、Glueがこのジョブを実行するために使用するハードウェアリソースを制御します。これは`DPU`という単位で表されます。DPU`の詳細については、[こちら](https://docs.aws.amazon.com/glue/latest/dg/add-job.html)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto_session.client(\"glue\")\n",
    "job_name = \"sparkml-abalone-\" + timestamp_prefix\n",
    "response = glue_client.create_job(\n",
    "    Name=job_name,\n",
    "    Description=\"PySpark job to featurize the Abalone dataset\",\n",
    "    Role=role,  # you can pass your existing AWS Glue role here if you have used Glue before\n",
    "    ExecutionProperty={\"MaxConcurrentRuns\": 1},\n",
    "    Command={\"Name\": \"glueetl\", \"ScriptLocation\": script_location},\n",
    "    DefaultArguments={\n",
    "        \"--job-language\": \"python\",\n",
    "        \"--extra-jars\": jar_dep_location,\n",
    "        \"--extra-py-files\": python_dep_location,\n",
    "    },\n",
    "    AllocatedCapacity=5,\n",
    "    Timeout=60,\n",
    ")\n",
    "glue_job_name = response[\"Name\"]\n",
    "print(glue_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前述のジョブは、`start_job_run` APIを呼び出すことで実行されます。このAPIは、上記で作成されたジョブ定義に対応する不変的なラン/実行を作成します。ステータスをチェックするために、特定のジョブ実行のための `job_run_id` を必要とします。データとモデルのロケーションは、ジョブ実行のパラメータの一部として渡します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run_id = glue_client.start_job_run(\n",
    "    JobName=job_name,\n",
    "    Arguments={\n",
    "        \"--S3_INPUT_BUCKET\": s3_input_bucket,\n",
    "        \"--S3_INPUT_KEY_PREFIX\": s3_input_key_prefix,\n",
    "        \"--S3_OUTPUT_BUCKET\": s3_output_bucket,\n",
    "        \"--S3_OUTPUT_KEY_PREFIX\": s3_output_key_prefix,\n",
    "        \"--S3_MODEL_BUCKET\": s3_model_bucket,\n",
    "        \"--S3_MODEL_KEY_PREFIX\": s3_model_key_prefix,\n",
    "    },\n",
    ")[\"JobRunId\"]\n",
    "print(job_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue Jobの状態を確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、ジョブのステータスをチェックして、`succeeded`、`failed`、`stopped`のどれかを確認します。ジョブが成功すると、変換されたデータがCSV形式でS3に入っているので、XGBoostでトレーニングに使用することができます。ジョブが失敗した場合は、[AWS Glue console](https://us-west-2.console.aws.amazon.com/glue/home)にアクセスして、左側の**Jobs**タブをクリックし、そのページからこの特定のジョブをクリックすると、これらのジョブのCloudWatchログ（**Logs**の下のリンク）リンクが表示され、ジョブの実行で何がうまくいかなかったかを確認するのに役立ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run_status = glue_client.get_job_run(JobName=job_name, RunId=job_run_id)[\"JobRun\"][\n",
    "    \"JobRunState\"\n",
    "]\n",
    "while job_run_status not in (\"FAILED\", \"SUCCEEDED\", \"STOPPED\"):\n",
    "    job_run_status = glue_client.get_job_run(JobName=job_name, RunId=job_run_id)[\"JobRun\"][\n",
    "        \"JobRunState\"\n",
    "    ]\n",
    "    print(job_run_status)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMakerのXGBoostを使って、SparkMLジョブで生成された処理済みデータセットでトレーニングを行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、SageMakerのXGBoostアルゴリズムを使って、このデータセットでトレーニングを行います。Glueジョブの一部として、前処理されたトレーニングデータがアップロードされたS3の場所はすでにわかっています。にアップロードされています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoostアルゴリズムの画像を取得する必要がある\n",
    "XGBoost内蔵アルゴリズムの画像を取得して、トレーニングに活用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, \"xgboost\", repo_version=\"latest\")\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 次のXGBoostモデルのパラメータとデータセットの詳細が適切に設定されます。\n",
    "PySparkスクリプトで使用したのと同じデータの場所をXGBoost Estimatorにも渡すことができるように、このノートブックをパラメータ化しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_data = \"s3://{}/{}/{}\".format(s3_output_bucket, s3_output_key_prefix, \"train\")\n",
    "s3_validation_data = \"s3://{}/{}/{}\".format(s3_output_bucket, s3_output_key_prefix, \"validation\")\n",
    "s3_output_location = \"s3://{}/{}/{}\".format(s3_output_bucket, s3_output_key_prefix, \"xgboost_model\")\n",
    "\n",
    "xgb_model = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.m5.xlarge\",\n",
    "    train_volume_size=20,\n",
    "    train_max_run=3600,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "xgb_model.set_hyperparameters(\n",
    "    objective=\"reg:linear\",\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    max_depth=5,\n",
    "    num_round=10,\n",
    "    subsample=0.7,\n",
    "    silent=0,\n",
    "    min_child_weight=6,\n",
    ")\n",
    "\n",
    "train_data = sagemaker.session.s3_input(\n",
    "    s3_train_data, distribution=\"FullyReplicated\", content_type=\"text/csv\", s3_data_type=\"S3Prefix\"\n",
    ")\n",
    "validation_data = sagemaker.session.s3_input(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最後にXGBoostトレーニングを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkMLとXGBoostモデルからなる推論パイプラインを構築し、リアルタイム推論のエンドポイントとする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、モデルをSageMakerにデプロイし、Inference Pipelineを作成します。Inference Pipelineは最大5つのコンテナで作成できます。\n",
    "\n",
    "SageMakerにモデルをデプロイするには、2つのコンポーネントが必要です。\n",
    "\n",
    "* ECR に存在する Docker イメージ。\n",
    "* S3に存在するモデルアーティファクト。\n",
    "\n",
    "**SparkML**の場合\n",
    "\n",
    "SparkMLについては、SageMakerチームがMLeapベースのSparkMLサービング用Dockerイメージを提供しています。詳細は[SageMaker SparkML Serving](https://github.com/aws/sagemaker-sparkml-serving-container)を参照してください。MLeapシリアライズされたSparkMLモデルは，AWS Glueで実行したSparkMLジョブの一部としてS3にアップロードされました．\n",
    "\n",
    "**XGBoost**について\n",
    "\n",
    "XGBoostでは、トレーニングに使用したものと同じDockerイメージを使用します。XGBoost用のモデルアーティファクトは、先ほど実行したトレーニングジョブの一部としてアップロードされました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ペイロードのスキーマを環境変数で渡す\n",
    "SparkML サービングコンテナは、`predict` メソッドを呼び出す際に、リクエストのスキーマを知る必要があります。リクエスト毎にスキーマを渡さなくても良いように、`sagemaker-sparkml-serving`ではモデル定義の作成時に環境変数を介してスキーマを渡すことができます。このスキーマ定義は、モデルを作成する次のステップで必要になります。\n",
    "\n",
    "このスキーマ定義は、個々のリクエストのペイロードの一部として渡すことで、リクエストごとに上書きすることができることを後ほど説明します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\"name\": \"sex\", \"type\": \"string\"},\n",
    "        {\"name\": \"length\", \"type\": \"double\"},\n",
    "        {\"name\": \"diameter\", \"type\": \"double\"},\n",
    "        {\"name\": \"height\", \"type\": \"double\"},\n",
    "        {\"name\": \"whole_weight\", \"type\": \"double\"},\n",
    "        {\"name\": \"shucked_weight\", \"type\": \"double\"},\n",
    "        {\"name\": \"viscera_weight\", \"type\": \"double\"},\n",
    "        {\"name\": \"shell_weight\", \"type\": \"double\"},\n",
    "    ],\n",
    "    \"output\": {\"name\": \"features\", \"type\": \"double\", \"struct\": \"vector\"},\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkMLとXGBoostのモデルを正しい順序で構成する`PipelineModel`の作成\n",
    "\n",
    "次に、SparkMLとXGBoostでSageMakerの`PipelineModel`を作成します。`PipelineModel`は、1つのAPIエンドポイントの後ろに両方のコンテナが正しい順番でデプロイされることを保証します。同じモデルは後にBatch Transformにも使用され、Pipelineに対する予測を行うには1つのジョブで十分であることを保証します。\n",
    "\n",
    "ここでは、SparkML用の`Model`の作成時に、前のセルで構築したスキーマ定義を渡します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "\n",
    "sparkml_data = \"s3://{}/{}/{}\".format(s3_model_bucket, s3_model_key_prefix, \"model.tar.gz\")\n",
    "# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\n",
    "sparkml_model = SparkMLModel(model_data=sparkml_data, env={\"SAGEMAKER_SPARKML_SCHEMA\": schema_json})\n",
    "xgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n",
    "\n",
    "model_name = \"inference-pipeline-\" + timestamp_prefix\n",
    "sm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### リアルタイム推論用のエンドポイントに`PipelineModel`をデプロイする\n",
    "次に、先ほど作成したモデルを `deploy()` メソッドで推論用のエンドポイントにデプロイし、エンドポイントにいくつかのリクエストを送信して、期待通りに動作するかどうかを検証します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"inference-pipeline-ep-\" + timestamp_prefix\n",
    "sm_model.deploy(initial_instance_count=1, instance_type=\"ml.c4.xlarge\", endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新しく作成した推論エンドポイントをペイロード付きで起動してデータを変換する\n",
    "次に、SageMaker SparkML Servingが認識できる有効なペイロードを使ってエンドポイントを呼び出します。入力ペイロードをリクエストに渡す方法は3つあります。\n",
    "\n",
    "* 有効なCSV文字列として渡す。この場合、環境変数を介して渡されたスキーマが決定に使用されます。CSV形式の場合，入力の各列は基本的なデータ型（int，double，stringなど）でなければならず，Sparkの`Array`や`Vector`であってはなりません。\n",
    "\n",
    "* 有効なJSON文字列として渡します。この場合も、環境変数で渡されたスキーマを推察して使用します。JSONフォーマットでは、入力の各カラムは、スキーマの対応するエントリに正しい値が記載されていれば、基本データ型やSparkの `Vector` または `Array` を使用できます。\n",
    "\n",
    "* JSON形式のリクエストをスキーマとデータと一緒に渡します。この場合、ペイロードで渡されたスキーマが、環境変数で渡されたスキーマよりも優先されます（もしあれば）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ペイロードのCSV形式での受け渡し\n",
    "まず、ペイロードをCSV形式でエンドポイントに渡す方法を説明します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import (\n",
    "    json_serializer,\n",
    "    csv_serializer,\n",
    "    json_deserializer,\n",
    "    RealTimePredictor,\n",
    ")\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "\n",
    "payload = \"F,0.515,0.425,0.14,0.766,0.304,0.1725,0.255\"\n",
    "predictor = RealTimePredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=csv_serializer,\n",
    "    content_type=CONTENT_TYPE_CSV,\n",
    "    accept=CONTENT_TYPE_CSV,\n",
    ")\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ペイロードをJSON形式で渡す\n",
    "ここでは、別のペイロードをJSON形式で渡してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"data\": [\"F\", 0.515, 0.425, 0.14, 0.766, 0.304, 0.1725, 0.255]}\n",
    "predictor = RealTimePredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=json_serializer,\n",
    "    content_type=CONTENT_TYPE_JSON,\n",
    "    accept=CONTENT_TYPE_CSV,\n",
    ")\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] スキーマとデータの両方を含むペイロードの受け渡し\n",
    "次に、スキーマとデータの両方で構成される入力ペイロードを渡します。よく見ると、このスキーマは環境変数で渡したものとは若干異なります。length`と`sex`のカラムの位置が入れ替わっているので、データも入れ替わっています。サーバはこのスキーマでペイロードを解析し、正常に動作するようになりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"schema\": {\n",
    "        \"input\": [\n",
    "            {\"name\": \"length\", \"type\": \"double\"},\n",
    "            {\"name\": \"sex\", \"type\": \"string\"},\n",
    "            {\"name\": \"diameter\", \"type\": \"double\"},\n",
    "            {\"name\": \"height\", \"type\": \"double\"},\n",
    "            {\"name\": \"whole_weight\", \"type\": \"double\"},\n",
    "            {\"name\": \"shucked_weight\", \"type\": \"double\"},\n",
    "            {\"name\": \"viscera_weight\", \"type\": \"double\"},\n",
    "            {\"name\": \"shell_weight\", \"type\": \"double\"},\n",
    "        ],\n",
    "        \"output\": {\"name\": \"features\", \"type\": \"double\", \"struct\": \"vector\"},\n",
    "    },\n",
    "    \"data\": [0.515, \"F\", 0.425, 0.14, 0.766, 0.304, 0.1725, 0.255],\n",
    "}\n",
    "\n",
    "predictor = RealTimePredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=json_serializer,\n",
    "    content_type=CONTENT_TYPE_JSON,\n",
    "    accept=CONTENT_TYPE_CSV,\n",
    ")\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Deleting the Endpoint\n",
    "このエンドポイントを使用する予定がない場合は、エンドポイントを削除して、エンドポイントを実行するためのコストを発生させないようにするのが良い方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto_session.client(\"sagemaker\")\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkMLとXGBoostモデルからなる推論パイプラインを構築し、1つのバッチトランスフォームジョブを実行する\n",
    "SageMaker Batch Transformでは、Inference Pipelineのデプロイ時に複数のコンテナを連結して、1つのバッチ変換ジョブを実行することもサポートしており、上記のリアルタイムユースケースと同様のバッチユースケースでデータを変換することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Transform用のデータを準備する\n",
    "Batch Transformでは、上記と同じフォーマットで、1行に1つのCSVまたはJSONのデータが必要です。このノートでは、SageMakerチームが、Batch Transformが処理できるCSV形式のサンプル入力を作成しました。この入力は基本的にトレーニングファイルと同様のCSVファイルですが、唯一の違いはラベル（``リング``）フィールドが含まれていないことです。\n",
    "\n",
    "次に、このデータのサンプルをSageMakerのバケットの1つからダウンロードして（名前は`batch_input_abalone.csv`）、S3バケットにアップロードします。また、ダウンロード後にデータの最初の5行を検査します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/batch_input_abalone.csv\n",
    "!printf \"\\n\\nShowing first five lines\\n\\n\"\n",
    "!head -n 5 batch_input_abalone.csv\n",
    "!printf \"\\n\\nAs we can see, it is identical to the training file apart from the label being absent here.\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_loc = sess.upload_data(\n",
    "    path=\"batch_input_abalone.csv\", bucket=default_bucket, key_prefix=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform API の起動による Batch Transform ジョブの作成\n",
    "次に、Python SDK の `Transformer` クラスを使用して、Batch Transform ジョブを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = \"s3://{}/{}/{}\".format(default_bucket, \"batch\", \"batch_input_abalone.csv\")\n",
    "output_data_path = \"s3://{}/{}/{}\".format(default_bucket, \"batch_output/abalone\", timestamp_prefix)\n",
    "job_name = \"serial-inference-batch-\" + timestamp_prefix\n",
    "transformer = sagemaker.transformer.Transformer(\n",
    "    # This was the model created using PipelineModel and it contains feature processing and XGBoost\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    strategy=\"SingleRecord\",\n",
    "    assemble_with=\"Line\",\n",
    "    output_path=output_data_path,\n",
    "    base_transform_job_name=\"serial-inference-batch\",\n",
    "    sagemaker_session=sess,\n",
    "    accept=CONTENT_TYPE_CSV,\n",
    ")\n",
    "transformer.transform(\n",
    "    data=input_data_path, job_name=job_name, content_type=CONTENT_TYPE_CSV, split_type=\"Line\"\n",
    ")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
