{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Model Monitor\n",
    "このノートブックは、次の方法を示しています。\n",
    "\n",
    "* Amazon SageMaker で機械学習モデルをホストし、推論リクエスト、結果、メタデータをキャプチャします\n",
    "* トレーニングデータセットを分析して、ベースライン制約を生成します\n",
    "* 制約に対する違反がないかライブエンドポイントを監視する\n",
    "\n",
    "---\n",
    "## Background\n",
    "\n",
    "Amazon SageMaker は、すべての開発者とデータサイエンティストに、機械学習モデルをすばやく構築、トレーニング、デプロイする機能を提供します。 Amazon SageMaker は、機械学習ワークフロー全体を網羅するフルマネージドサービスです。 データにラベルを付けて準備し、アルゴリズムを選択し、モデルをトレーニングしてから、展開用に調整して最適化することができます。 Amazon SageMaker を使用してモデルを本番環境にデプロイし、以前よりも予測と低コストを実現できます。\n",
    "\n",
    "さらに、Amazon SageMaker を使用すると、デプロイするモデルの呼び出しの入力、出力、メタデータをキャプチャできます。 また、データを分析してその品質を監視することもできます。 このノートブックでは、Amazon SageMaker がこれらの機能をどのように有効にするかを学びます。\n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "開始するには、これらの前提条件が満たされていることを確認してください。\n",
    "\n",
    "* モデルをホストする AWSリージョン を指定します。\n",
    "* Amazon SageMaker に Amazon Simple Storage Service（Amazon S3） のデータへのアクセスを許可するために使用される IAMロールARN が存在します。 必要な権限を調整する方法については、ドキュメントを参照してください。\n",
    "* モデルのトレーニングに使用されるデータ、追加のモデルデータ、およびモデル呼び出しからキャプチャされたデータを格納するために使用される S3バケット を作成します。 デモンストレーションの目的で、これらに同じバケットを使用しています。 実際には、それらを異なるセキュリティポリシーで分離することをお勧めします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考資料\n",
    "\n",
    "- [公式ドキュメント](https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/model-monitor.html)\n",
    "- [Amazon SageMaker Model Monitor を活用したデータドリフト検知の解説](https://aws.amazon.com/jp/blogs/news/detect-data-drift-with-amazon-sagemaker-model-monitor/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Handful of configuration\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "from sagemaker import get_execution_role, session\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))\n",
    "\n",
    "# You can use a different bucket, but make sure the role you chose for this notebook\n",
    "# has the s3:PutObject permissions. This is the bucket into which the data is captured\n",
    "bucket = session.Session(boto3.Session()).default_bucket()\n",
    "print(\"Demo Bucket: {}\".format(bucket))\n",
    "prefix = \"sagemaker/DEMO-ModelMonitor\"\n",
    "\n",
    "data_capture_prefix = \"{}/datacapture\".format(prefix)\n",
    "s3_capture_upload_path = \"s3://{}/{}\".format(bucket, data_capture_prefix)\n",
    "reports_prefix = \"{}/reports\".format(prefix)\n",
    "s3_report_path = \"s3://{}/{}\".format(bucket, reports_prefix)\n",
    "code_prefix = \"{}/code\".format(prefix)\n",
    "s3_code_preprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"preprocessor.py\")\n",
    "s3_code_postprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"postprocessor.py\")\n",
    "\n",
    "print(\"Capture path: {}\".format(s3_capture_upload_path))\n",
    "print(\"Report path: {}\".format(s3_report_path))\n",
    "print(\"Preproc Code path: {}\".format(s3_code_preprocessor_uri))\n",
    "print(\"Postproc Code path: {}\".format(s3_code_postprocessor_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックの実行ロールに続行するために必要な権限があることをすばやく確認できます。 上記で指定したS3バケットに簡単なテストオブジェクトを配置します。 このコマンドが失敗した場合は、バケットに対する `s3：PutObject` 権限を持つようにロールを更新して、再試行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload some test files\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\"test_upload/test.txt\").upload_file(\n",
    "    \"test_data/upload-test-file.txt\"\n",
    ")\n",
    "print(\"Success! You are all set to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A: Amazon SageMakerエンドポイントからのリアルタイム推論データの取得\n",
    "エンドポイントを作成して、データキャプチャー機能の動作を紹介します。\n",
    "\n",
    "###  学習済みのモデルをAmazon S3にアップロードする\n",
    "このコードは、事前にトレーニングされたXGBoostモデルをアップロードして、すぐに展開できるようにします。このモデルは、SageMaker の XGB Churn Prediction Notebook を使用してトレーニングされています。このステップでは、独自の事前学習済みモデルを使用することもできます。すでにAmazon S3に事前学習済みのモデルがある場合は、s3_keyを指定することで代わりに追加することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = open(\"model/xgb-churn-prediction-model.tar.gz\", \"rb\")\n",
    "s3_key = os.path.join(prefix, \"xgb-churn-prediction-model.tar.gz\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(s3_key).upload_fileobj(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMakerにモデルをデプロイする\n",
    "まずは、事前に学習された解約予測モデルの展開から始めます。ここでは、画像とモデルデータを含むモデルオブジェクトを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "model_name = \"DEMO-xgb-churn-pred-model-monitor-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = \"https://{}.s3-{}.amazonaws.com/{}/xgb-churn-prediction-model.tar.gz\".format(\n",
    "    bucket, region, prefix\n",
    ")\n",
    "\n",
    "image_uri = retrieve(\"xgboost\", boto3.Session().region_name, \"0.90-1\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=model_url, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルのデータ品質を監視するためにデータキャプチャーを有効にするには、`DataCaptureConfig`という新しいキャプチャーオプションを指定します。この設定では、リクエストのペイロード、レスポンスのペイロード、またはその両方をキャプチャすることができます。キャプチャーコンフィグはすべてのバリアントに適用されます。デプロイメントを実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "endpoint_name = \"DEMO-xgb-churn-pred-model-monitor-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName={}\".format(endpoint_name))\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デプロイされたモデルを呼び出す\n",
    "\n",
    "これで、このエンドポイントにデータを送信して、リアルタイムで推論を得ることができます。前のステップでデータキャプチャーを有効にしたので、リクエストとレスポンスのペイロードは、いくつかの追加メタデータとともに、DataCaptureConfigで指定したAmazon Simple Storage Service（Amazon S3）のロケーションに保存されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このステップでは、サンプルデータを含むエンドポイントを約3分間起動します。データは指定されたサンプリング・パーセンテージに基づいてキャプチャーされ、データ・キャプチャー・オプションがオフになるまでキャプチャーが続けられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import time\n",
    "\n",
    "predictor = Predictor(endpoint_name=endpoint_name, serializer=CSVSerializer())\n",
    "\n",
    "# get a subset of test data for a quick test\n",
    "!head -180 test_data/test-dataset-input-cols.csv > test_data/test_sample.csv\n",
    "print(\"Sending test traffic to the endpoint {}. \\nPlease wait...\".format(endpoint_name))\n",
    "\n",
    "with open(\"test_data/test_sample.csv\", \"r\") as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip(\"\\n\")\n",
    "        response = predictor.predict(data=payload)\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## キャプチャしたデータを見る\n",
    "\n",
    "ここで、Amazon S3に保存されているデータキャプチャファイルをリストアップしてみましょう。呼び出しが発生した時間に基づいて整理された、異なる時間帯の異なるファイルが表示されることを期待してください。Amazon S3のパスのフォーマットは以下の通りです。\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "current_endpoint_capture_prefix = \"{}/{}\".format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、1つのキャプチャファイルの内容を表示します。ここでは、Amazon SageMaker固有のJSONライン形式のファイルにキャプチャされたすべてのデータが表示されます。キャプチャファイルの最初の数行を簡単に覗いてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get(\"Body\").read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、1行の内容を整形されたJSONファイルで以下に示しますので、少しでもよく観察してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(json.loads(capture_file.split(\"\\n\")[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ご覧の通り、各推論要求はjsonlファイルの1行に収められています。この行には入力と出力の両方が一緒にマージされています。この例では、ContentTypeを`text/csv`としており、`observedContentType`の値に反映されています。また、入力と出力のペイロードをキャプチャ形式でエンコードするのに使用したエンコーディングを `encoding` の値で公開しています。\n",
    "\n",
    "要約すると、新しいパラメータを使って、エンドポイントへの入力ペイロードまたは出力ペイロードのキャプチャを有効にする方法を確認しました。また、キャプチャされたフォーマットがAmazon S3でどのように見えるかを観察しました。次に、Amazon S3で収集されたデータを監視するために、Amazon SageMakerがどのように役立つかを引き続き調べてみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B: Model Monitor - ベースラインと継続的モニタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMakerは、データを収集するだけでなく、エンドポイントが観測したデータを監視・評価する機能も提供しています。これには\n",
    "\n",
    "1. リアルタイムのトラフィックと比較するためのベースラインを作成します。\n",
    "1. ベースラインの準備ができたら、継続的に評価し、ベースラインと比較するためのスケジュールを設定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ベースライン/トレーニングデータセットを用いた制約条件の提案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルをトレーニングしたトレーニングデータセットは、通常、良いベースラインデータセットです。トレーニングデータセットのデータスキーマと推論データセットのスキーマは完全に一致している必要があることに注意してください（つまり、特徴の数と順序）。\n",
    "\n",
    "トレーニングデータセットから、Amazon SageMakerにベースラインの「制約」のセットを提案してもらい、データを探索するための記述的な「統計」を生成することができます。この例では、この例に含まれるプレトレーニングモデルをトレーニングするために使用されたトレーニングデータセットをアップロードします。すでにAmazon S3にある場合は、それを直接指定することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over the training dataset to Amazon S3 (if you already have it in Amazon S3, you could reuse it)\n",
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = \"s3://{}/{}\".format(bucket, baseline_data_prefix)\n",
    "baseline_results_uri = \"s3://{}/{}\".format(bucket, baseline_results_prefix)\n",
    "print(\"Baseline data uri: {}\".format(baseline_data_uri))\n",
    "print(\"Baseline results uri: {}\".format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file = open(\"test_data/training-dataset-with-header.csv\", \"rb\")\n",
    "s3_key = os.path.join(baseline_prefix, \"data\", \"training-dataset-with-header.csv\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(s3_key).upload_fileobj(training_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニングデータセットを使ったベースライニングジョブの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3にトレーニングデータの準備ができたので、制約を `suggest` するジョブを開始します。`DefaultModelMonitor.suggest_baseline(...)`は、制約を生成するためにAmazon SageMakerが提供するModel Monitorコンテナを使用して、`ProcessingJob`を開始します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri + \"/training-dataset-with-header.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成された制約条件や統計情報を調べる\n",
    "\n",
    "- statiistics.json  \n",
    "  このファイルには、分析対象のデータセット内の各フィーチャに対する列状の統計が含まれます  \n",
    "- constraints.json  \n",
    "  このファイルには、確認されたフィーチャの制約が含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スケジュールの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データの質に問題がないかどうか収集したデータを分析\n",
    "\n",
    "上記のデータを収集したら、モニタリング・スケジュールでデータを分析・監視する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.io.json.json_normalize(\n",
    "    baseline_job.suggested_constraints().body_dict[\"features\"]\n",
    ")\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まず、いくつかのテストスクリプトをS3バケットにコピーして、前後の処理に使用できるようにします。\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(code_prefix + \"/preprocessor.py\").upload_file(\n",
    "    \"preprocessor.py\"\n",
    ")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(code_prefix + \"/postprocessor.py\").upload_file(\n",
    "    \"postprocessor.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先に作成したエンドポイントのモデル監視スケジュールを作成することができます。ベースラインのリソース（制約条件や統計情報）を使用して、リアルタイムのトラフィックと比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "mon_schedule_name = \"DEMO-xgb-churn-pred-model-monitor-schedule-\" + strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\", gmtime()\n",
    ")\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint, # モニタリング対象のエンドポイント\n",
    "    # record_preprocessor_script=pre_processor_script,\n",
    "    post_analytics_processor_script=s3_code_postprocessor_uri, # 後処理するスクリプトをS3に事前に用意しておいて、そのパスを指定する。\n",
    "    output_s3_uri=s3_report_path, # モニタリング結果を出力するS3パス\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 人工的なトラフィックの生成を開始\n",
    "\n",
    "以下のセルは、エンドポイントにいくつかのトラフィックを送信するスレッドを開始します。このスレッドを終了させるには、カーネルを停止する必要があることに注意してください。トラフィックがない場合、処理するデータがないため、監視ジョブは `Failed` とマークされます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "endpoint_name = predictor.endpoint\n",
    "runtime_client = boto3.client(\"runtime.sagemaker\")\n",
    "\n",
    "# (just repeating code from above for convenience/ able to run this section independently)\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for row in f:\n",
    "            payload = row.rstrip(\"\\n\")\n",
    "            response = runtime_client.invoke_endpoint(\n",
    "                EndpointName=ep_name, ContentType=\"text/csv\", Body=payload\n",
    "            )\n",
    "            response[\"Body\"].read()\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        invoke_endpoint(endpoint_name, \"test_data/test-dataset-input-cols.csv\", runtime_client)\n",
    "\n",
    "\n",
    "thread = Thread(target=invoke_endpoint_forever)\n",
    "thread.start()\n",
    "\n",
    "# Note that you need to stop the kernel to stop the invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スケジュールの説明と点検\n",
    "\n",
    "記述したら、MonitoringScheduleStatusがScheduledに変わることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print(\"Schedule status: {}\".format(desc_schedule_result[\"MonitoringScheduleStatus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行内容の一覧\n",
    "スケジュールでは、あらかじめ指定した間隔でジョブを開始します。ここでは、最新の5つの実行結果が表示されています。1時間ごとのスケジュールを作成した後にこれを起動する場合、実行内容が空欄になっていることがありますのでご注意ください。実行の開始を確認するには、（UTCの）時間の境界を越えるまで待たなければならないかもしれません。以下のコードは、待機するためのロジックです。\n",
    "\n",
    "注：1時間ごとのスケジュールであっても、Amazon SageMakerは実行をスケジュールするために20分のバッファ期間を持っています。実行の開始時間は、1時間の境界から0分から20分の間になることがあります。これは、バックエンドでのロードバランシングのために行われます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\n",
    "    \"We created a hourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\\nWe will have to wait till we hit the hour...\"\n",
    ")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特定の実行（最新の実行）の検査\n",
    "前のセルでは、最新の完了または失敗したスケジュール実行をピックアップしました。ここでは、考えられるターミナルの状態と、それぞれの意味を説明します。\n",
    "\n",
    "* Completed - モニタリングの実行が完了し、違反レポートに問題が見つからなかったことを意味します。\n",
    "* CompletedWithViolations - これは、実行が完了したが、制約違反が検出されたことを意味します。\n",
    "* Failed - モニタリングの実行が失敗したことを意味します。原因は、クライアントのエラー（おそらく間違ったロールのプレミッション）またはインフラストラクチャの問題です。何が起こったかを正確に特定するには、FailureReasonとExitMessageのさらなる調査が必要です。\n",
    "* Stopped - ジョブが最大ランタイムを超えたか、手動で停止されました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = mon_executions[\n",
    "    -1\n",
    "]  # latest execution's index is -1, second to last is -2 and so on..\n",
    "time.sleep(60)\n",
    "latest_execution.wait(logs=False)\n",
    "\n",
    "print(\"Latest execution status: {}\".format(latest_execution.describe()[\"ProcessingJobStatus\"]))\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()[\"ExitMessage\"]))\n",
    "\n",
    "latest_job = latest_execution.describe()\n",
    "if latest_job[\"ProcessingJobStatus\"] != \"Completed\":\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_uri = latest_execution.output.destination\n",
    "print(\"Report Uri: {}\".format(report_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成されたレポートの一覧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip(\"/\")\n",
    "print(\"Report bucket: {}\".format(report_bucket))\n",
    "print(\"Report key: {}\".format(report_key))\n",
    "\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 違反レポート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベースラインと比較して違反があった場合は、ここに記載されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option(\"display.max_colwidth\", -1)\n",
    "constraints_df = pd.io.json.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### その他のコマンド\n",
    "また、監視スケジュールの開始や停止も可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_default_monitor.stop_monitoring_schedule()\n",
    "# my_default_monitor.start_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リソースの削除\n",
    "\n",
    "データの収集を継続するために、エンドポイントの実行を継続することができます。これ以上データを収集したり、このエンドポイントを使用したりする予定がない場合は、追加料金の発生を避けるために、エンドポイントを削除してください。なお、エンドポイントを削除しても、モデルの起動時に取得したデータは削除されません。そのデータは、あなた自身が削除するまで、Amazon S3に残ります。\n",
    "\n",
    "しかしその前に、まずスケジュールを削除する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor.delete_monitoring_schedule()\n",
    "time.sleep(60)  # actually wait for the deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
